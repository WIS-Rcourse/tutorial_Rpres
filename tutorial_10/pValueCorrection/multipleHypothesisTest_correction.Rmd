---
title: "MHT p value correction"
date: "21/12/2021"
output:
  rmdformats::downcute:
        highlight: kate
        self_contained: true
        code_folding: show
        thumbnails: false
        gallery: true
        df_print: paged
        lightbox: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, collapse=T,include =T)
```

**Author: [Miguel García](https://angelcampos.github.io/)**

[Multiple hypothesis testing](https://en.wikipedia.org/wiki/Multiple_comparisons_problem#Definition) 
(MHT) occurs when a statistical analysis involves multiple simultaneous statistical
tests, each of which has a potential to produce a "discovery" (namely, rejecting
the null hypothesis), on the **same dataset**. A common example in the Life Sciences field arose when
we were able to measure multiple genes in the same experiment and we wanted to
know which genes were differentially expressed between control and a treatment
groups; but it could apply to any experiment in which we obtain measurements
for multiple features and we want to perform a statistical test for each of the 
features. Nevertheless when performing MHT a problem arises.

The so called [Multiple comparison problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem),
a.k.a. [Look-elsewhere effect](https://en.wikipedia.org/wiki/Look-elsewhere_effect),
is that **the more inferences are made, the more likely erroneous**
**inferences are to occur**. Several statistical techniques have been developed 
to prevent this from happening, allowing significance levels for single and 
multiple comparisons to be directly compared. These techniques generally 
require a stricter significance threshold for individual comparisons,
to compensate for the number of inferences being made. We can also see this 
methods as a way to **correct the p Values** obtained from the MHT procedure.
To further understand the multiple comparison problem I have prepared the 
following material to simulate MHT experiments and their outcomes when we are 
expecting normally distributed values (similar approaches would apply to data 
from other theorethical distributions).

# Multiple Hypothesis, why correcting?

Multiple Hypothesis Testing (MHT) p Value correction becomes evident when 
simulating experiments with known distributions and their outcome.

The MHE() function bellow simulates a MHT experiment, creates **nFeatures**
sampling **nSample** "measurements" from two *normal distributions* with means **popMean1** 
and **popMean2** with a standard deviation of **popSD** for both cases; then 
it calculates a p Value with a t-test for each feature. The output of the function
is a dataframe of the resulting p Values for each feature in the column **raw**,
which stands for p Values that have not been corrected.

This function will help us create artificial data and p Values here on.

```{r}
MHE <- function(nFeatures, nSample, popMean1, popMean2, popSD){
  data.frame(raw = unlist(lapply(1:nFeatures, function(x){
    smp1 <- rnorm(nSample, popMean1, popSD)
    smp2 <- rnorm(nSample, popMean2, popSD)
    t.test(smp1, smp2)$p.value
  })))
}
```

In the following example we create 10'000 features with both theoretical populations 
having the same mean value of 100, which would be akin to measure ten thousand genes 
in treatment and control, when the treatment has no effect at all, and test for differences

> Gene counts from RNA-seq methods do not distribut normally, currently one 
of the theoretical distributions to model gene counts is the [Negative binomial
distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution) 
but for practical purposes here we use the normal distribution.

In theory the samples should have the same mean and there should be no 
differences, but due to chance we can pick measurements that are very different
from one another and seem that they come from different distributions. When
applying a t-test to such samples we will obtain a small p-value. Using
a statistical threshold \alpha = 0.05 we guard against false postives 95%
of the times, when performing only one inference.

> *For practical purposes we are using the same mean for all feature simulations
but in real experiments the mean for each feature can be different yet we will
have the same outcome.*

```{r}
set.seed(2020) # Seed for reproducibility purposes
nFeat <- 1e4 # Number of features
pVals <- MHE(nFeatures = nFeat, nSample = 10, popMean1 = 100, popMean2 = 100, popSD = 10)
```

Plotting the distribution of the p values we can appreciate that they distribute
uniformly, we have almost the same chance of having a p-value between 0 and 0.1
and between 0.9 and 1. 

```{r, fig.height = 3}
hist(pVals$raw, main = "Histogram of p Values")
```

If we check the proportion of tests with a p Value bellow a
singnificance threshold of \alpha = 0.05, we will observe that it is close to 5%,
which is similar to our alpha. It should become now evident how a p Value tries 
to control for False positives, as due to chance we can obtain a discovery 
where there should not be.

```{r}
alpha <- 0.05
sum(pVals$raw < alpha) / nFeat
```

By ploting the Empirical Cumulative Distribution Function (ECDF) of the p values
distribution, we appreciate that is like a straight line from 0,0 to 1,1. 
This indicates that there is roughly the same chance of falling at any interval.

```{r, fig.height = 3}
plot(ecdf(pVals$raw), main = "ECDF of p Values")
```

If we wanted to know the probability of our MHT experiments that will have false 
discoveries using \alpha and no correction we would be calculating the 
[Family-wise Error Rate (FWER)](https://en.wikipedia.org/wiki/Family-wise_error_rate)

The FWER, is given by the formula:

$$ FWER = 1 - (1 - \alpha )^m  $$

When  ***m*** independent comparisons are performed at a **\alpha** confidence level,

In our original case with 10'000 tests FWER = 1 - 1.722078e-223, which means
that virtually our experiment will ALWAYS have false discoveries; and when 
there are too many false discoveries it will be hard to set a threshold to tell
which ones are real and which are not.

Having false discoveries is undesirable but more so is never having the ability
to come with plaussible statistically validated discoveries, which prompts the 
need to control for this theoretical impasse.

# Family-wise Error Rate (FWER) control

## Bonferroni correction method

The Bonferroni correction attempts to bring the FWER to the initial confidence
level value. So that only 5% of our experiments run with MHT have false discoveries.

To achieve this each test is evaluated at a confidence value divided by the
number of tests. Which is equivalent to multiply the p values by the number
of tests (with an upper threshold of 1, as p Values cannot be higher than 1).

Then we can evaluate at the original confidence value. Bellow we can define 
a function that implements the Bonferroni correction.

```{r}
bonferroni_correction <- function(p){
  pTmp <- p * length(p)
  pTmp <- sapply(pTmp, function(x){
    min(x, 1)
  })
  return(pTmp)
}
```

And then apply it to our "raw" p Values.

```{r}
pVals$bonferroni <- bonferroni_correction(pVals$raw)
sum(pVals$bonferroni < 0.05)
```

Now we have `r sum(pVals$bonferroni < 0.05)` p values that are smaller than
the confidence level. Which is good as in our original example there
are no difference between sample populations. By using the Bonferroni 
correction method we have made it less likely to have a false discovery, we 
would need to perform hundreds of MHT experiments to again reach to the 
expected 5% chance of having at least one false discovery in the whole experiment.

But what if we truly have different means in the population? We can run the
same simulation but with popMean1 = 100 and popMean2 = 120, a relatively large 
difference, and then apply the bonferroni correction to prevent false discoveries.

> Remember: In these simulations we know that the means of the populations are 
indeed different but in real experiments we don't, so we still would need to
apply a p Value correction method.

```{r}
pVals2 <- MHE(nFeatures = nFeat, nSample = 10, popMean1 = 100, popMean2 = 120, popSD = 10)
pVals2$bonferroni <- bonferroni_correction(pVals2$raw)
mean(pVals2$bonferroni < 0.05)
```

In this example, around 8% of the tests are able to reject the null hypothesis 
after using the Bonferroni correction. **When in reality all of them should have**
**been rejected!**

Bonferroni correction is too conservative and losing so many true discoveries
is not desirable on many instances, specially if obtaining the data requires
a lot of resources, as Life Sciences research. That is why other methods
have to be developed.

## Hochberg correction method

Yosef Hochber in [1988](http://www-stat.wharton.upenn.edu/~steele/Courses/956/Resource/MultipleComparision/Hochberg88.pdf)
presented a slightly more powerful procedure (able to detect more real differences
than bonferroni) in which the p values are evaluated from biggest to 
smallest. 

The biggest p value is evaluated with the original confidence level (α),
if the p value is smaller then all subsequent hypothesis are rejected, 
if not the next biggest p-value is tested with a confidence level of α/2,
and so on.

This would be similar to multiplying the smallest p value by the number of 
tests, then the second p value by the number of tests minus 1, and so on
until this operation yields a bigger number than the biggest original p value
at which point all subsequent p values will be assigned that same p value.
Again we can implement this function using R!

```{r}
hochberg_correction <- function(p){
  oNames <- names(p) # save the original names if there are
  names(p) <- seq_along(p)
  pTmp <- sort(p, decreasing = FALSE)
  pTmp <- pTmp * (length(p):1)
  pTmp[which(pTmp > max(p))] <- max(p)
  pAdj <- pTmp[order(as.numeric(names(pTmp)))] #Sort p values in original order
  names(pAdj) <- oNames #Original names (if any)
  return(pAdj)
}
```

Now we can use Hochberg's correction method and see the proportion of rejected
hypothesis

```{r}
pVals$hochberg <- hochberg_correction(pVals$raw) 
pVals2$hochberg <- hochberg_correction(pVals2$raw) 
mean(pVals2$hochberg < 0.05)
```

We had an improvement, from ~8% to ~9% rejected null hypothesis. Still
too conservative given that the measurements were taken from
reasonably different distributions. So there should be a way to be able to detect
more TRUE differences, but still correcting for the influence of doing 
multiple testing. Maybe FWER control is too strict.

# False Discovery Rate (FDR) control

Both Bonferroni's and Hochberg's FWER control procedures are directed towards
controlling the chance of having **at least one** false discovery. This method 
of statistical control has proven too stringent and in many cases makes 
impossible to detect plaussibly real differences in measurements. Reason
why we needed other, perhaps less stringent, methods.

What if we could be less stringent, but yet have a statistical basis to control
for the proportion of false discoveries? Or in other words the 
[False Discovery Rate](https://en.wikipedia.org/wiki/False_discovery_rate)
(FDR).

FDR-controlling procedures are designed to control the expected proportion of
discoveries that are false positives (incorrect rejections of the 
null hypothesis) in a MHT procedure. These methods provide less stringent 
control of False Positives compared to FWER-controlling methods, but have
greater power.

## Benjamini-Hochberg method (FDR method)

The FDR concept was formally described by Yoav Benjamini and Yosef Hochberg in
[1995](https://www.jstor.org/stable/2346101?seq=4#metadata_info_tab_contents)
as a practical and powerfull approach at identifying the important few
from the trivial many effects tested.

The Benjamini-Hochberg FDR method has been particularly influential, as it was
the first alternative to the FWER methods to gain broad acceptance in many 
scientific fields, especially in Genomics, where experiments measuring hundreds
to thousands of genes started to appear.

Altough the underlying theory can be daunting, the actual mathematical procedure
is quite simple. And implemented in the function bellow.

1. Order p-values from smallest to largest.
2. Give a rank to the p-values (1 to smallest, n to largest).
3. The largest p value stays the same.
3. The next largest adjusted p-value is the smaller of two options:
  a) The previous adjusted p value.
  b) The original p value multiplied by the number of p Values divided by the rank
  of the p value we are adjusting.

> A nice illustation of the underlying logic can be found in a video by Josh Starmer 
(aka StatQuest) found on the references section at the end of this document.

```{r}
# Benjamin Hochberg FDR correction implementation
BH_correction <- function(p){
  oNames <- names(p) # save the original names if there are
  names(p) <- seq_along(p)
  pTmp <- sort(p, decreasing = FALSE)
  rank <- seq_along(pTmp)
  for(i in rev(head(rank, -1))){
    pA <- pTmp[i + 1] # The previous corrected p value
    pB <- pTmp[i] * length(p) / rank[i] # p value * number of p Values / rank
    pTmp[i] <- min(c(pA, pB)) # The minimum of both
  }
  pAdj <- pTmp[order(as.numeric(names(pTmp)))]
  names(pAdj) <- oNames
  return(pAdj)
}
```

The resulting adjusted p values are called **q Values**. Once we have them we can apply 
a threshold, which would be the rate of expected false discoveries. For 
example a common used threshold for q Values is 0.1. If we then reject hypothesis
that have a smaller q Value, we will expect to have 10% of false discoveries, 
and 90% of true discoveries.

If we apply the FDR correction to our two previous examples 1) samples coming
from the same distribution, and 2) samples coming from two different distributions,
we can appreciate two effects: We have succesfully 
eliminated the false discoveries in the case where there shouldn't be null 
hypothesis rejection; and in the case were there are differences we now are 
able to recover most of the real differences.

```{r}
pVals$FDR <- BH_correction(pVals$raw)
pVals2$FDR <- BH_correction(pVals2$raw)

FDR1 <- mean(pVals$FDR < 0.05)
FDR2 <- mean(pVals2$FDR < 0.05)
```

---

Proportion of Q values bellow 0.05, when the samples are extracted from the **same** distribution = `r FDR1`

Proportion of Q values bellow 0.05, when the samples are extracted from **different** distribution = `r FDR2`

---

We can see the effect of the corrected p values in the form of boxplots
shown bellow. We apply a -10log-transformation to see small p Values more
easily, and draw a line which indicates the hypothesis that pass the 
threshold \alpha = 0.05. (Smaller p Values have bigger -log10 values)

```{r, fig.width= 10}
boxplot(-log10(pVals), main = "Samples from same distribution")
abline(h = -log10(0.05), col = "blue", las = 2)
```

As we can see many tests reject the null hypothesis when we don't make the 
correction, even when samples come from the same distribution, just by chance!
But after correcting this effect is lost.

In the second example we sample from two different distributions, in which in
principle we should reject the null hypothesis all the time, but when applying
FWER controling methods we end up "over-correcting" thus having very few
discoveries. However when using FDR controlling methods we still keep most of the 
TRUE positives.

```{r}
boxplot(-log10(pVals2), main = "Samples from different distributions")
abline(h = -log10(0.05), col = "blue", las = 2)
```

# Epilogue

As R is a statistically oriented Programming language it already comes 
with a function that performs p Value correction. Which is
`p.adjust()`.

You can correct pValues with different methods as stated in the help of the 
function, using the `method` argument. Bellow examples of how to perform each
of the corrections done before.

```{r, eval = FALSE}
p.adjust(pVals$raw, method = "bonferroni")
p.adjust(pVals$raw, method = "hochberg") 
p.adjust(pVals$raw, method = "fdr") # Also, p.adjust(pVals$raw, method = "BH")
```

I hope that this (hopefully brief) tutorial and the implementation of some p 
Value correction methods help you understand the reasons why we perform, 
how it is done, what we lose and what we gain by performing each of them. 

> A special note has to be made with the use of the \alpha = 0.05 threshold.
Here is only used for illustrative purposes,
the interpretaion of a p Value **passing or not passing a certain**
**threshold** for a result being "statistically significant" should should not 
be done without care and blind to the size effects. A more thorough discussion
in this matter can be followed in the 2nd most popular article of 2019, as 
ranked by Altmetric [Scientists rise up against statistical significance](https://www.altmetric.com/top100/2019/?details=57358237).

# References

* Hochberg, Yosef. "A sharper Bonferroni procedure for multiple tests of 
significance." Biometrika 75.4 (1988): 800-802. [URL](https://academic.oup.com/biomet/article/75/4/800/423177?login=true)
* Benjamini, Yoav, and Yosef Hochberg. "Controlling the false discovery rate: a practical and powerful approach to
multiple testing." Journal of the Royal statistical society: series B (Methodological) 57.1 (1995): 289-300. 
[URL](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x)
* [Wikipedia: Family-wise Error Rate](https://en.wikipedia.org/wiki/Family-wise_error_rate)
* [Wikipedia: False Dicovery Rate](https://en.wikipedia.org/wiki/False_discovery_rate)
* [Recommended video: False Discovery Rates, FDR, clearly explained by Statquest](https://youtu.be/K8LQSvtjcEo)
