---
title: "MHT p value correction"
date: "21/12/2021"
output:
  rmdformats::downcute:
        highlight: kate
        self_contained: true
        code_folding: show
        thumbnails: false
        gallery: true
        df_print: paged
        lightbox: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, collapse=T,include =T)
```

**Author: [Miguel García](https://angelcampos.github.io/)**

# Multiple Hypothesis, why correcting?

One of the main points on why to perform Multiple Hypothesis Testing (MHT) p Value
correction becomes evident when simulating experiments and their outcome.

In tutorial #10 I showed you how you can do the following:

1) Generate two samples from the same theoretical normal distribution.
2) Test for differences on the mean in either direction from both samples, using
the [Student's t-test](https://en.wikipedia.org/wiki/Student's_t-test).

In theory the samples should have the same mean and there should be no 
differences, but due to chance we can pick measurements that are very different
from one another and seem that they come from different distributions, or populations.

The MHE() function bellow creates *nFeatures* sampling *nSample* "measurements"
from two normal distributions *popMean1* and *popMean2*  with a standard 
deviation of *popSD* in both cases; then calculates a p Value with a t-test. 
This function will help us create artificial data and p Values.

```{r}
MHE <- function(nFeatures, nSample, popMean1, popMean2, popSD){
  data.frame(raw = unlist(lapply(1:nFeatures, function(x){
    smp1 <- rnorm(nSample, popMean1, popSD)
    smp2 <- rnorm(nSample, popMean2, popSD)
    t.test(smp1, smp2)$p.value
  })))
}
```

In the following example we create 10000 features with both populations 
having a mean value of 100, which would be akin to measure ten thousand genes in treatment and control and
test for differences *(If genes expression distributed normally, which they DON'T)*.

> *For practical purposes we are using the same mean for all feature simulations
but in real experiments the mean for each feature can be different yet we will
have the same outcome.*

```{r}
set.seed(2020)
nFeat <- 1e4 # Number of features
pVals <- MHE(nFeatures = nFeat, nSample = 10, popMean1 = 100, popMean2 = 100, popSD = 10)
```

If we check the proportion of tests with a p Value bellow a
singnificance threshold of \alpha = 0.05, we will observe that it is close to 5%.

```{r}
alpha <- 0.05
sum(pVals$raw < alpha) / nFeat
```

Plotting the distribution of the p values we can appreciate that they distribute
uniformly, we have almost the same chance of having a p-value between 0 and 0.1
and between 0.9 and 1. 

```{r, fig.height = 3}
hist(pVals$raw, main = "Histogram of p Values")
```

By ploting the Empirical Cumulative Distribution Function (ECDF) of the p values
distribution, we appreciate that is like a straight line from 0,0 to 1,1, 
which means that there is roughly the same chance of falling at any interval.

```{r, fig.height = 3}
plot(ecdf(pVals$raw), main = "ECDF of p Values")
```

At the 5% confidence level running 100 independent tests we will expect on average
around 0.05 x 20 = 5 false discoveries, even when there are no effects whatsoever.
What happens when we perform much more tests? How many times would we have
at least one false discovery? 

If m independent comparisons are performed at a 0.05 confidence level,
the family-wise error rate (FWER), is given by the formula

$$ FWER = 1 - (1 - 0.05 )^m  $$

In our original case with 10'000 tests FWER = 1 - 1.722078e-223, which means
that virtually our experiment will ALWAYS have false discoveries. 
Having false discoveries is undesirable but more so is never having the ability
to come with plaussible statistically validated discoveries, which prompts the 
need to control for this theoretical impasse.

# FWER controlling

## Bonferroni Correction (1936)

The Bonferroni correction attempts to bring the FWER to the initial confidence
level value. So that only 5% of experiments run with multiple hypothesis testing
have false discoveries.

To achieve this each test is evaluated at a confidence value divided by the
number of tests. Which is equivalent to multiply the p values by the number
of tests (with an upper threshold of 1, as p Values cannot be higher than 1).

Then we can evaluate at the original confidence value. Bellow we can define 
a function that implements the Bonferroni correction.

```{r}
bonferroni_correction <- function(p){
  pTmp <- p * length(p)
  pTmp <- sapply(pTmp, function(x){
    min(x, 1)
  })
  return(pTmp)
}
```


```{r}
pVals$bonferroni <- bonferroni_correction(pVals$raw)
sum(pVals$bonferroni < 0.05)
```

Now we have `r sum(pVals$bonferroni < 0.05)` p values that are smaller than
the confidence level. Which is good as in our original example there
are no difference between sample populations.

But what if we truly have different means in the population? We can run the
same simulation but with popMean1 = 100 and popMean2 = 120, a relatively large 
difference. 

```{r}
pVals2 <- MHE(nFeatures = nFeat, nSample = 10, popMean1 = 100, popMean2 = 120, popSD = 10)
pVals2$bonferroni <- bonferroni_correction(pVals2$raw)
mean(pVals2$bonferroni < 0.05)
```

In this example, around 8% of the tests are able to reject the null hypothesis 
after using the Bonferroni correction. When in reality all of them should have 
been rejected!

Bonferroni correction is too conservative and losing so many true discoveries
is not desirable on many instances, as Life Sciences research in which millions
of dollars are spent to find therapeutic targets.

## Hochberg (1988)

Yosef Hochber in [1988](http://www-stat.wharton.upenn.edu/~steele/Courses/956/Resource/MultipleComparision/Hochberg88.pdf)
presented a slightly more powerful procedure (able to detect real differences
than bonferroni) in which the p values are evaluated from biggest to 
smallest. 

The biggest p value is evaluated with the original confidence level (α),
if the p value is smaller then all subsequent hypothesis are rejected, 
if not the next biggest p-value is tested with a confidence level of α/2,
and so on.

This would be similar to multiplying the smallest p value by the number of 
tests, then the second p value by the number of tests minus 1, and so on
until this operation yields a bigger number than the biggest original p value
at which point all subsequent p values will be assigned that same p value.
Again we can implement this function using R!

```{r}
hochberg_correction <- function(p){
  oNames <- names(p) # save the original names if there are
  names(p) <- seq_along(p)
  pTmp <- sort(p, decreasing = FALSE)
  pTmp <- pTmp * (length(p):1)
  pTmp[which(pTmp > max(p))] <- max(p)
  pAdj <- pTmp[order(as.numeric(names(pTmp)))] #Sort p values in original order
  names(pAdj) <- oNames #Original names (if any)
  return(pAdj)
}
```

Now we can use Hochberg's correction method and see the proportion of rejected
hypothesis

```{r}
pVals2$hochberg <- hochberg_correction(pVals2$raw) 
mean(pVals2$hochberg < 0.05)
```

We had an improvement, from ~8% to ~9% rejected null hypothesis. But still
too conservative given that the measurements were taken from
really different distributions. So there should be a way to be able to detect
more TRUE differences, but still correcting for the influence of doing 
multiple testing.

# False Discovery Rate (FDR)

Both Bonferroni's and Hochberg's FWER control procedures are directed towards
controlling the chance of having **at least one** false discovery. This method 
of statistical control has proven too stringent and in many cases makes 
impossible to detect plaussibly real differences in measurements. Reason
why we need less stringent methods.

What if we could be less stringent, but yet have a statistical basis to control
for the proportion of false discoveries?

FDR-controlling procedures are designed to control the expected proportion of
discoveries that are false positives (incorrect rejections of the 
null hypothesis). Provide less stringent control of False Positives
compared to FWER controlling procedures, but have greater power.

## Benjamini-Hochberg 1995

The FDR concept was formally described by Yoav Benjamini and Yosef Hochberg in
[1995](https://www.jstor.org/stable/2346101?seq=4#metadata_info_tab_contents)
as a practical and powerfull approach at identifying the important few
from the trivial many effects tested.

The Benjamini-Hochberg FDR method has been particularly influential, as it was
the first alternative to the FWER methods to gain broad acceptance in many 
scientific fields, especially in Genomics, where experiments measuring thousands
of genes started to appear in the late 90's early 2000's.

Altough the underlying theory can be daunting, the actual mathematical procedure
is quite simple. And implemented in the function bellow.

1. Order p-values from smallest to largest.
2. Give a rank to the p-values. 1 to smallest n to largest.
3. The largest p value stays the same
3. The next largest adjusted p-value is the smaller of two options:
  a) The previous adjusted p value.
  b) The original p value multiplied by the number of p Values divided by the rank
  of the p value we are adjusting.

> A nice graphic illustation of this procedure can be found in a video by 
StatQuest found on the references section at the end of this document.

```{r}
# Benjamin Hochberg FDR correction implementation
BH_correction <- function(p){
  oNames <- names(p) # save the original names if there are
  names(p) <- seq_along(p)
  pTmp <- sort(p, decreasing = FALSE)
  rank <- seq_along(pTmp)
  for(i in rev(head(rank, -1))){
    pA <- pTmp[i + 1] # The previous corrected p value
    pB <- pTmp[i] * length(p) / rank[i] # p value * number of p Values / rank
    pTmp[i] <- min(c(pA, pB)) # The minimum of both
  }
  pAdj <- pTmp[order(as.numeric(names(pTmp)))]
  names(pAdj) <- oNames
  return(pAdj)
}
```

The resulting adjusted p values are called **q Values**. Once we have them we can apply 
a threshold, which would be the rate of expected False discoveries. For 
example a common used threshold for q Values is 0.1. If we then reject hypothesis
that have a smaller q Value, we will expect to have 10% of false discoveries.

If we apply the FDR correction to our two previous examples 1) No real difference,
2) Different distributions, we can appreciate that we have succesfully 
eliminated the false discoveries in the case where there shouldn't be null 
hypothesis rejection. And in the case were there are differences we now were 
able to recover most of the real differences.

```{r}
pVals$FDR <- BH_correction(pVals$raw)
pVals2$FDR <- BH_correction(pVals2$raw)

FDR1 <- mean(pVals$FDR < 0.05)
FDR2 <- mean(pVals2$FDR < 0.05)
```

Proportion of Q values bellow 0.05, when the samples are extracted from the **same** distribution = `r FDR1`

Proportion of Q values bellow 0.05, when the samples are extracted from **different** distribution = `r FDR2`

We can see the effect of the corrected p values in the form of boxplots
shown bellow. We apply a -10log-transformation to see small p Values more
easily, and draw a line which indicates the hypothesis that pass the 
threshold \alpha = 0.05. (Smaller p Values have bigger -log10 values)

```{r, fig.width= 10}
boxplot(-log10(pVals), main = "Samples from same distribution")
abline(h = -log10(0.05), col = "blue", las = 2)
```

As we can see many tests reject the null hypothesis when we don't make the 
correction, even when samples come from the same distribution, just by chance!

In the second example we sample from two different distributions, in which in
principle we should reject the null hypothesis all the time, but when applying
FWER controling methods we end up "over-correcting" thus having very few
discoveries. However when using FDR controlling methods we still keep most of the 
TRUE positives.

```{r}
boxplot(-log10(pVals2), main = "Samples from different distributions")
abline(h = -log10(0.05), col = "blue", las = 2)
```

# Epilogue

As R is a statistically oriented Programming language it already comes 
with a function that performs p Value correction. Which is
`p.adjust()`.

You can correct pValues with different methods as stated in the help of the 
function, using the `method` argument. Bellow examples of how to perform each
of the corrections done before.

```{r, eval = FALSE}
p.adjust(pVals$raw, method = "bonferroni")
p.adjust(pVals$raw, method = "hochberg") 
p.adjust(pVals$raw, method = "fdr") # Also, p.adjust(pVals$raw, method = "BH")
```

I hope that this tutorial and the implementation of some p Value correction
methods help you understand the reasons why we perform, what we lose and what
we gain by performing each of them. 

A special note has to be made with the use of the \alpha = 0.05 threshold, 
here is only used for illustrative purposes,
the interpretaion of a p Value **passing or not passing a certain**
**threshold** for a result being "statistically significant" should should not 
be done without care and blind to the size effects. A more thorough discussion
in this matter can be followed in one of the most popular articles in 2019 as 
ranked by Altmetric [Scientists rise up against statistical significance](https://www.altmetric.com/top100/2019/?details=57358237).

# References

* Hochberg, Yosef. "A sharper Bonferroni procedure for multiple tests of 
significance." Biometrika 75.4 (1988): 800-802. [URL](https://academic.oup.com/biomet/article/75/4/800/423177?login=true)
* Benjamini, Yoav, and Yosef Hochberg. "Controlling the false discovery rate: a practical and powerful approach to
multiple testing." Journal of the Royal statistical society: series B (Methodological) 57.1 (1995): 289-300. 
[URL](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x)
* [Wikipedia: Family-wise Error Rate](https://en.wikipedia.org/wiki/Family-wise_error_rate)
* [Wikipedia: False Dicovery Rate](https://en.wikipedia.org/wiki/False_discovery_rate)
* [Video: False Discovery Rates, FDR, clearly explained by Statquest](https://youtu.be/K8LQSvtjcEo)
